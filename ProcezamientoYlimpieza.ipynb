{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9f4624",
   "metadata": {},
   "source": [
    "# An√°lisis de Precios de Autom√≥viles - Procesamiento y Modelado\n",
    "\n",
    "Este notebook implementa un pipeline completo de procesamiento de datos y entrenamiento de modelos para predecir precios de autom√≥viles.\n",
    "\n",
    "## Contenido:\n",
    "1. **Procesamiento y limpieza de datos**\n",
    "   - Manejo de valores nulos\n",
    "   - Codificaci√≥n de variables categ√≥ricas  \n",
    "   - Normalizaci√≥n/estandarizaci√≥n\n",
    "   - Reducci√≥n de dimensionalidad\n",
    "   - Pipeline de scikit-learn\n",
    "   - Divisi√≥n train/val/test (70/15/15)\n",
    "\n",
    "2. **Entrenamiento de modelos**\n",
    "   - k-Nearest Neighbors (kNN)\n",
    "   - Random Forest (modelo de ensamble)\n",
    "   - Deep Neural Network (DNN) con 3+ capas ocultas\n",
    "   - Evaluaci√≥n comparativa en train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f552a0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Usando CPU para TensorFlow\n",
      "‚úÖ TensorFlow importado correctamente\n",
      "‚ùå Error al importar SciKeras - instalando...\n",
      "‚úÖ Todas las librer√≠as importadas correctamente\n",
      "üìä Pandas version: 2.3.2\n",
      "üî¢ NumPy version: 2.3.3\n",
      "ü§ñ TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n inicial para suprimir warnings\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar TensorFlow para evitar warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suprimir warnings de TensorFlow\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Evitar warnings de oneDNN\n",
    "\n",
    "# Importaci√≥n de librer√≠as b√°sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Librer√≠as de scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Configuraci√≥n para matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Importar TensorFlow despu√©s de la configuraci√≥n\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    # Configuraciones adicionales de TensorFlow\n",
    "    tf.get_logger().setLevel('ERROR')  # Solo mostrar errores\n",
    "    \n",
    "    # Verificar si hay GPU disponible (opcional)\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        print(\"üéÆ GPU disponible para TensorFlow\")\n",
    "        # Configurar crecimiento de memoria GPU para evitar conflictos\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    else:\n",
    "        print(\"üíª Usando CPU para TensorFlow\")\n",
    "    \n",
    "    # Importar componentes de Keras despu√©s de configurar TensorFlow\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    \n",
    "    print(\"‚úÖ TensorFlow importado correctamente\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå Error al importar TensorFlow\")\n",
    "\n",
    "# Importar scikeras para integraci√≥n con scikit-learn\n",
    "try:\n",
    "    from scikeras.wrappers import KerasRegressor\n",
    "    print(\"‚úÖ SciKeras importado correctamente\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Error al importar SciKeras - instalando...\")\n",
    "\n",
    "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
    "print(f\"ü§ñ TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd375ed",
   "metadata": {},
   "source": [
    "## 1. Carga y Exploraci√≥n Inicial de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7e850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (205, 26)\n",
      "Columnas: ['car_ID', 'symboling', 'CarName', 'fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight', 'enginetype', 'cylindernumber', 'enginesize', 'fuelsystem', 'boreratio', 'stroke', 'compressionratio', 'horsepower', 'peakrpm', 'citympg', 'highwaympg', 'price']\n",
      "\n",
      "=== INFORMACI√ìN GENERAL ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 205 entries, 0 to 204\n",
      "Data columns (total 26 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   car_ID            205 non-null    int64  \n",
      " 1   symboling         205 non-null    int64  \n",
      " 2   CarName           205 non-null    object \n",
      " 3   fueltype          205 non-null    object \n",
      " 4   aspiration        205 non-null    object \n",
      " 5   doornumber        205 non-null    object \n",
      " 6   carbody           205 non-null    object \n",
      " 7   drivewheel        205 non-null    object \n",
      " 8   enginelocation    205 non-null    object \n",
      " 9   wheelbase         205 non-null    float64\n",
      " 10  carlength         205 non-null    float64\n",
      " 11  carwidth          205 non-null    float64\n",
      " 12  carheight         205 non-null    float64\n",
      " 13  curbweight        205 non-null    int64  \n",
      " 14  enginetype        205 non-null    object \n",
      " 15  cylindernumber    205 non-null    object \n",
      " 16  enginesize        205 non-null    int64  \n",
      " 17  fuelsystem        205 non-null    object \n",
      " 18  boreratio         205 non-null    float64\n",
      " 19  stroke            205 non-null    float64\n",
      " 20  compressionratio  205 non-null    float64\n",
      " 21  horsepower        205 non-null    int64  \n",
      " 22  peakrpm           205 non-null    int64  \n",
      " 23  citympg           205 non-null    int64  \n",
      " 24  highwaympg        205 non-null    int64  \n",
      " 25  price             205 non-null    float64\n",
      "dtypes: float64(8), int64(8), object(10)\n",
      "memory usage: 41.8+ KB\n",
      "\n",
      "=== PRIMERAS 5 FILAS ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 205 entries, 0 to 204\n",
      "Data columns (total 26 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   car_ID            205 non-null    int64  \n",
      " 1   symboling         205 non-null    int64  \n",
      " 2   CarName           205 non-null    object \n",
      " 3   fueltype          205 non-null    object \n",
      " 4   aspiration        205 non-null    object \n",
      " 5   doornumber        205 non-null    object \n",
      " 6   carbody           205 non-null    object \n",
      " 7   drivewheel        205 non-null    object \n",
      " 8   enginelocation    205 non-null    object \n",
      " 9   wheelbase         205 non-null    float64\n",
      " 10  carlength         205 non-null    float64\n",
      " 11  carwidth          205 non-null    float64\n",
      " 12  carheight         205 non-null    float64\n",
      " 13  curbweight        205 non-null    int64  \n",
      " 14  enginetype        205 non-null    object \n",
      " 15  cylindernumber    205 non-null    object \n",
      " 16  enginesize        205 non-null    int64  \n",
      " 17  fuelsystem        205 non-null    object \n",
      " 18  boreratio         205 non-null    float64\n",
      " 19  stroke            205 non-null    float64\n",
      " 20  compressionratio  205 non-null    float64\n",
      " 21  horsepower        205 non-null    int64  \n",
      " 22  peakrpm           205 non-null    int64  \n",
      " 23  citympg           205 non-null    int64  \n",
      " 24  highwaympg        205 non-null    int64  \n",
      " 25  price             205 non-null    float64\n",
      "dtypes: float64(8), int64(8), object(10)\n",
      "memory usage: 41.8+ KB\n",
      "\n",
      "=== PRIMERAS 5 FILAS ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_ID</th>\n",
       "      <th>symboling</th>\n",
       "      <th>CarName</th>\n",
       "      <th>fueltype</th>\n",
       "      <th>aspiration</th>\n",
       "      <th>doornumber</th>\n",
       "      <th>carbody</th>\n",
       "      <th>drivewheel</th>\n",
       "      <th>enginelocation</th>\n",
       "      <th>wheelbase</th>\n",
       "      <th>...</th>\n",
       "      <th>enginesize</th>\n",
       "      <th>fuelsystem</th>\n",
       "      <th>boreratio</th>\n",
       "      <th>stroke</th>\n",
       "      <th>compressionratio</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>peakrpm</th>\n",
       "      <th>citympg</th>\n",
       "      <th>highwaympg</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>alfa-romero giulia</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>convertible</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>88.6</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111</td>\n",
       "      <td>5000</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>13495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>alfa-romero stelvio</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>convertible</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>88.6</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.47</td>\n",
       "      <td>2.68</td>\n",
       "      <td>9.0</td>\n",
       "      <td>111</td>\n",
       "      <td>5000</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>alfa-romero Quadrifoglio</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>two</td>\n",
       "      <td>hatchback</td>\n",
       "      <td>rwd</td>\n",
       "      <td>front</td>\n",
       "      <td>94.5</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>2.68</td>\n",
       "      <td>3.47</td>\n",
       "      <td>9.0</td>\n",
       "      <td>154</td>\n",
       "      <td>5000</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>16500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>audi 100 ls</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>fwd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.8</td>\n",
       "      <td>...</td>\n",
       "      <td>109</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>10.0</td>\n",
       "      <td>102</td>\n",
       "      <td>5500</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>13950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>audi 100ls</td>\n",
       "      <td>gas</td>\n",
       "      <td>std</td>\n",
       "      <td>four</td>\n",
       "      <td>sedan</td>\n",
       "      <td>4wd</td>\n",
       "      <td>front</td>\n",
       "      <td>99.4</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>mpfi</td>\n",
       "      <td>3.19</td>\n",
       "      <td>3.40</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115</td>\n",
       "      <td>5500</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>17450.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   car_ID  symboling                   CarName fueltype aspiration doornumber  \\\n",
       "0       1          3        alfa-romero giulia      gas        std        two   \n",
       "1       2          3       alfa-romero stelvio      gas        std        two   \n",
       "2       3          1  alfa-romero Quadrifoglio      gas        std        two   \n",
       "3       4          2               audi 100 ls      gas        std       four   \n",
       "4       5          2                audi 100ls      gas        std       four   \n",
       "\n",
       "       carbody drivewheel enginelocation  wheelbase  ...  enginesize  \\\n",
       "0  convertible        rwd          front       88.6  ...         130   \n",
       "1  convertible        rwd          front       88.6  ...         130   \n",
       "2    hatchback        rwd          front       94.5  ...         152   \n",
       "3        sedan        fwd          front       99.8  ...         109   \n",
       "4        sedan        4wd          front       99.4  ...         136   \n",
       "\n",
       "   fuelsystem  boreratio  stroke compressionratio horsepower  peakrpm citympg  \\\n",
       "0        mpfi       3.47    2.68              9.0        111     5000      21   \n",
       "1        mpfi       3.47    2.68              9.0        111     5000      21   \n",
       "2        mpfi       2.68    3.47              9.0        154     5000      19   \n",
       "3        mpfi       3.19    3.40             10.0        102     5500      24   \n",
       "4        mpfi       3.19    3.40              8.0        115     5500      18   \n",
       "\n",
       "   highwaympg    price  \n",
       "0          27  13495.0  \n",
       "1          27  16500.0  \n",
       "2          26  16500.0  \n",
       "3          30  13950.0  \n",
       "4          22  17450.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv('CarPrice_Assignment.csv')\n",
    "print(f\"Dimensiones del dataset: {df.shape}\")\n",
    "print(f\"Columnas: {df.columns.tolist()}\")\n",
    "\n",
    "# Mostrar informaci√≥n b√°sica\n",
    "print(\"\\n=== INFORMACI√ìN GENERAL ===\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n=== PRIMERAS 5 FILAS ===\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27df3a",
   "metadata": {},
   "source": [
    "## 2. Procesamiento y Limpieza de Datos\n",
    "\n",
    "### 2.1 Manejo de Valores Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4dd67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de valores nulos\n",
    "print(\"=== AN√ÅLISIS DE VALORES NULOS ===\")\n",
    "null_counts = df.isnull().sum()\n",
    "null_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "null_summary = pd.DataFrame({\n",
    "    'Columna': df.columns,\n",
    "    'Valores_Nulos': null_counts.values,\n",
    "    'Porcentaje_Nulos': null_percentages.values\n",
    "})\n",
    "\n",
    "print(null_summary[null_summary['Valores_Nulos'] > 0])\n",
    "\n",
    "# Si no hay valores nulos, crear algunos de ejemplo para mostrar el manejo\n",
    "if null_summary['Valores_Nulos'].sum() == 0:\n",
    "    print(\"‚úÖ No se encontraron valores nulos en el dataset\")\n",
    "else:\n",
    "    print(f\"‚ùå Se encontraron {null_summary['Valores_Nulos'].sum()} valores nulos en total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4e1f94",
   "metadata": {},
   "source": [
    "### 2.2 Identificaci√≥n de Variables Categ√≥ricas y Num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04bc007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar tipos de variables\n",
    "target_column = 'price'\n",
    "\n",
    "# Separar variables categ√≥ricas y num√©ricas\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Remover la variable objetivo de las variables num√©ricas\n",
    "if target_column in numerical_columns:\n",
    "    numerical_columns.remove(target_column)\n",
    "\n",
    "# Remover variables no √∫tiles (como ID)\n",
    "if 'car_ID' in numerical_columns:\n",
    "    numerical_columns.remove('car_ID')\n",
    "\n",
    "print(\"=== CLASIFICACI√ìN DE VARIABLES ===\")\n",
    "print(f\"Variables categ√≥ricas ({len(categorical_columns)}): {categorical_columns}\")\n",
    "print(f\"Variables num√©ricas ({len(numerical_columns)}): {numerical_columns}\")\n",
    "print(f\"Variable objetivo: {target_column}\")\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\n=== ESTAD√çSTICAS DESCRIPTIVAS - VARIABLES NUM√âRICAS ===\")\n",
    "df[numerical_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f9dcb",
   "metadata": {},
   "source": [
    "### 2.3 An√°lisis de Variables Categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4782027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de variables categ√≥ricas\n",
    "print(\"=== AN√ÅLISIS DE VARIABLES CATEG√ìRICAS ===\")\n",
    "for col in categorical_columns:\n",
    "    unique_values = df[col].nunique()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  - Valores √∫nicos: {unique_values}\")\n",
    "    print(f\"  - Valores: {df[col].value_counts().head().to_dict()}\")\n",
    "    if unique_values < 10:\n",
    "        print(f\"  - Distribuci√≥n completa: {df[col].value_counts().to_dict()}\")\n",
    "\n",
    "# Analizar cardinalidad para decidir estrategia de codificaci√≥n\n",
    "categorical_info = pd.DataFrame({\n",
    "    'Variable': categorical_columns,\n",
    "    'Cardinalidad': [df[col].nunique() for col in categorical_columns],\n",
    "    'Tipo_Sugerido': ['OneHot' if df[col].nunique() <= 10 else 'Target/Label' for col in categorical_columns]\n",
    "})\n",
    "\n",
    "print(\"\\n=== ESTRATEGIA DE CODIFICACI√ìN ===\")\n",
    "categorical_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02180805",
   "metadata": {},
   "source": [
    "### 2.4 Pipeline de Procesamiento con Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir X e y\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "print(f\"Forma de X: {X.shape}\")\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "\n",
    "# Pipeline para variables num√©ricas\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Imputaci√≥n con mediana (m√°s robusta)\n",
    "    ('scaler', StandardScaler())  # Estandarizaci√≥n\n",
    "])\n",
    "\n",
    "# Pipeline para variables categ√≥ricas\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputaci√≥n con moda\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # One-hot encoding\n",
    "])\n",
    "\n",
    "# Combinador de pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_columns),\n",
    "    ('cat', categorical_pipeline, categorical_columns)\n",
    "])\n",
    "\n",
    "# Evaluar si necesitamos reducci√≥n de dimensionalidad\n",
    "total_features_after_encoding = len(numerical_columns)\n",
    "for col in categorical_columns:\n",
    "    total_features_after_encoding += df[col].nunique()\n",
    "\n",
    "print(f\"\\n=== AN√ÅLISIS DE DIMENSIONALIDAD ===\")\n",
    "print(f\"Features num√©ricas: {len(numerical_columns)}\")\n",
    "print(f\"Features categ√≥ricas (originales): {len(categorical_columns)}\")\n",
    "print(f\"Features estimadas despu√©s de One-Hot: {total_features_after_encoding}\")\n",
    "\n",
    "# Decidir si usar PCA\n",
    "use_pca = total_features_after_encoding > 50  # Umbral para usar PCA\n",
    "print(f\"¬øUsar PCA? {use_pca} (umbral: >50 features)\")\n",
    "\n",
    "# Pipeline completo\n",
    "if use_pca:\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('pca', PCA(n_components=0.95))  # Mantener 95% de la varianza\n",
    "    ])\n",
    "    print(\"‚úÖ Pipeline creado CON reducci√≥n de dimensionalidad (PCA)\")\n",
    "else:\n",
    "    full_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor)\n",
    "    ])\n",
    "    print(\"‚úÖ Pipeline creado SIN reducci√≥n de dimensionalidad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc48705d",
   "metadata": {},
   "source": [
    "### 2.5 Divisi√≥n de Datos (70/15/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥n estratificada de datos: 70% train, 15% val, 15% test\n",
    "print(\"=== DIVISI√ìN DE DATOS ===\")\n",
    "\n",
    "# Primero separar train+val (85%) del test (15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.15, \n",
    "    random_state=42,\n",
    "    stratify=None  # Para regresi√≥n no usamos stratify\n",
    ")\n",
    "\n",
    "# Luego separar train (70% del total) del val (15% del total)\n",
    "# 15/85 ‚âà 0.176 para obtener 15% del dataset original\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.176,  # 15% del dataset original\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset completo: {X.shape[0]} muestras\")\n",
    "print(f\"Train: {X_train.shape[0]} muestras ({X_train.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(f\"Validation: {X_val.shape[0]} muestras ({X_val.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "print(f\"Test: {X_test.shape[0]} muestras ({X_test.shape[0]/X.shape[0]*100:.1f}%)\")\n",
    "\n",
    "# Verificar distribuci√≥n de la variable objetivo\n",
    "print(f\"\\n=== DISTRIBUCI√ìN DE LA VARIABLE OBJETIVO ===\")\n",
    "print(f\"Train - Media: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"Val - Media: {y_val.mean():.2f}, Std: {y_val.std():.2f}\")\n",
    "print(f\"Test - Media: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a640430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar el pipeline de procesamiento\n",
    "print(\"=== APLICANDO PIPELINE DE PROCESAMIENTO ===\")\n",
    "\n",
    "# Ajustar el pipeline con datos de entrenamiento y transformar todos los conjuntos\n",
    "X_train_processed = full_pipeline.fit_transform(X_train)\n",
    "X_val_processed = full_pipeline.transform(X_val)\n",
    "X_test_processed = full_pipeline.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Datos procesados exitosamente\")\n",
    "print(f\"Forma despu√©s del procesamiento:\")\n",
    "print(f\"  - X_train: {X_train_processed.shape}\")\n",
    "print(f\"  - X_val: {X_val_processed.shape}\")\n",
    "print(f\"  - X_test: {X_test_processed.shape}\")\n",
    "\n",
    "# Si se us√≥ PCA, mostrar informaci√≥n adicional\n",
    "if use_pca:\n",
    "    pca = full_pipeline.named_steps['pca']\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumsum_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    print(f\"\\n=== INFORMACI√ìN DEL PCA ===\")\n",
    "    print(f\"Componentes principales: {pca.n_components_}\")\n",
    "    print(f\"Varianza explicada por los primeros 5 componentes: {explained_variance_ratio[:5].round(3)}\")\n",
    "    print(f\"Varianza total explicada: {cumsum_variance[-1]:.3f}\")\n",
    "    \n",
    "    # Gr√°fico de varianza explicada\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, 'bo-')\n",
    "    plt.title('Varianza Explicada por Componente')\n",
    "    plt.xlabel('Componente Principal')\n",
    "    plt.ylabel('Varianza Explicada')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(cumsum_variance) + 1), cumsum_variance, 'ro-')\n",
    "    plt.title('Varianza Explicada Acumulada')\n",
    "    plt.xlabel('N√∫mero de Componentes')\n",
    "    plt.ylabel('Varianza Explicada Acumulada')\n",
    "    plt.grid(True)\n",
    "    plt.axhline(y=0.95, color='k', linestyle='--', alpha=0.7, label='95%')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a0b3e2",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento y Evaluaci√≥n de Modelos\n",
    "\n",
    "### 3.1 Funci√≥n de Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a15f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para evaluar modelos de manera consistente\n",
    "def evaluate_model(model, X_train, X_val, X_test, y_train, y_val, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Eval√∫a un modelo en los conjuntos de train, validaci√≥n y test\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== EVALUANDO MODELO: {model_name} ===\")\n",
    "    \n",
    "    # Predicciones\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # M√©tricas para cada conjunto\n",
    "    results = {\n",
    "        'Modelo': model_name,\n",
    "        'Train_RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'Train_MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "        'Train_R2': r2_score(y_train, y_train_pred),\n",
    "        'Val_RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "        'Val_MAE': mean_absolute_error(y_val, y_val_pred),\n",
    "        'Val_R2': r2_score(y_val, y_val_pred),\n",
    "        'Test_RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'Test_MAE': mean_absolute_error(y_test, y_test_pred),\n",
    "        'Test_R2': r2_score(y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"Train - RMSE: {results['Train_RMSE']:.2f}, MAE: {results['Train_MAE']:.2f}, R¬≤: {results['Train_R2']:.3f}\")\n",
    "    print(f\"Val   - RMSE: {results['Val_RMSE']:.2f}, MAE: {results['Val_MAE']:.2f}, R¬≤: {results['Val_R2']:.3f}\")\n",
    "    print(f\"Test  - RMSE: {results['Test_RMSE']:.2f}, MAE: {results['Test_MAE']:.2f}, R¬≤: {results['Test_R2']:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de evaluaci√≥n definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e4b30",
   "metadata": {},
   "source": [
    "### 3.2 Modelo 1: k-Nearest Neighbors (kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37680e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 1: k-Nearest Neighbors\n",
    "print(\"üî∏ ENTRENANDO MODELO kNN\")\n",
    "\n",
    "# Probar diferentes valores de k para encontrar el √≥ptimo\n",
    "k_values = [3, 5, 7, 9, 11]\n",
    "best_k = 5\n",
    "best_val_score = float('inf')\n",
    "\n",
    "print(\"Buscando el mejor valor de k...\")\n",
    "for k in k_values:\n",
    "    knn_temp = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn_temp.fit(X_train_processed, y_train)\n",
    "    val_pred = knn_temp.predict(X_val_processed)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    print(f\"k={k}: RMSE validaci√≥n = {val_rmse:.2f}\")\n",
    "    \n",
    "    if val_rmse < best_val_score:\n",
    "        best_val_score = val_rmse\n",
    "        best_k = k\n",
    "\n",
    "print(f\"‚úÖ Mejor k encontrado: {best_k}\")\n",
    "\n",
    "# Entrenar modelo final con el mejor k\n",
    "knn_model = KNeighborsRegressor(n_neighbors=best_k)\n",
    "knn_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Evaluar modelo\n",
    "knn_results = evaluate_model(\n",
    "    knn_model, X_train_processed, X_val_processed, X_test_processed,\n",
    "    y_train, y_val, y_test, f\"kNN (k={best_k})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc146b",
   "metadata": {},
   "source": [
    "### 3.3 Modelo 2: Random Forest (Modelo de Ensamble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 2: Random Forest\n",
    "print(\"üå≤ ENTRENANDO MODELO RANDOM FOREST\")\n",
    "\n",
    "# Entrenar Random Forest con hiperpar√°metros optimizados\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,      # N√∫mero de √°rboles\n",
    "    max_depth=15,          # Profundidad m√°xima\n",
    "    min_samples_split=5,   # M√≠nimo muestras para dividir\n",
    "    min_samples_leaf=2,    # M√≠nimo muestras en hoja\n",
    "    max_features='sqrt',   # N√∫mero de features a considerar\n",
    "    random_state=42,\n",
    "    n_jobs=-1             # Usar todos los cores disponibles\n",
    ")\n",
    "\n",
    "print(\"Entrenando Random Forest...\")\n",
    "rf_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Evaluar modelo\n",
    "rf_results = evaluate_model(\n",
    "    rf_model, X_train_processed, X_val_processed, X_test_processed,\n",
    "    y_train, y_val, y_test, \"Random Forest\"\n",
    ")\n",
    "\n",
    "# Importancia de features (si no se us√≥ PCA)\n",
    "if not use_pca:\n",
    "    feature_importance = rf_model.feature_importances_\n",
    "    # Como usamos ColumnTransformer, necesitamos obtener los nombres de features\n",
    "    feature_names = []\n",
    "    \n",
    "    # Features num√©ricas\n",
    "    feature_names.extend(numerical_columns)\n",
    "    \n",
    "    # Features categ√≥ricas (despu√©s de one-hot encoding)\n",
    "    cat_feature_names = full_pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_columns)\n",
    "    feature_names.extend(cat_feature_names)\n",
    "    \n",
    "    # Crear DataFrame con importancias\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n=== TOP 10 FEATURES M√ÅS IMPORTANTES (Random Forest) ===\")\n",
    "    print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121585f",
   "metadata": {},
   "source": [
    "### 3.4 Modelo 3: Deep Neural Network (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 3: Deep Neural Network (DNN)\n",
    "print(\"üß† ENTRENANDO DEEP NEURAL NETWORK\")\n",
    "\n",
    "def create_dnn_model(input_dim):\n",
    "    \"\"\"\n",
    "    Crea un modelo DNN con m√≠nimo 3 capas ocultas, \n",
    "    funciones de activaci√≥n y regularizaci√≥n\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Capa de entrada + primera capa oculta\n",
    "        Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Segunda capa oculta\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Tercera capa oculta\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Cuarta capa oculta (adicional)\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Capa de salida (regresi√≥n)\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    # Compilar modelo\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Obtener dimensi√≥n de entrada\n",
    "input_dim = X_train_processed.shape[1]\n",
    "print(f\"Dimensi√≥n de entrada: {input_dim}\")\n",
    "\n",
    "# Crear modelo\n",
    "dnn_model = create_dnn_model(input_dim)\n",
    "\n",
    "# Mostrar arquitectura\n",
    "print(\"\\n=== ARQUITECTURA DEL MODELO DNN ===\")\n",
    "dnn_model.summary()\n",
    "\n",
    "# Callback para early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nüîÑ Entrenando DNN...\")\n",
    "# Entrenar modelo\n",
    "history = dnn_model.fit(\n",
    "    X_train_processed, y_train,\n",
    "    validation_data=(X_val_processed, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo DNN\n",
    "dnn_results = evaluate_model(\n",
    "    dnn_model, X_train_processed, X_val_processed, X_test_processed,\n",
    "    y_train, y_val, y_test, \"Deep Neural Network\"\n",
    ")\n",
    "\n",
    "# Visualizar el entrenamiento\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('P√©rdida del Modelo DNN')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Error Absoluto Medio (MAE)')\n",
    "plt.xlabel('√âpocas')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚èπÔ∏è Entrenamiento detenido en √©poca: {len(history.history['loss'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d29561",
   "metadata": {},
   "source": [
    "### 3.5 Tabla Comparativa de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c8ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla comparativa de resultados\n",
    "print(\"üìä TABLA COMPARATIVA DE RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Crear DataFrame con todos los resultados\n",
    "results_df = pd.DataFrame([knn_results, rf_results, dnn_results])\n",
    "\n",
    "# Reordenar columnas para mejor visualizaci√≥n\n",
    "column_order = [\n",
    "    'Modelo', \n",
    "    'Train_RMSE', 'Val_RMSE', 'Test_RMSE',\n",
    "    'Train_MAE', 'Val_MAE', 'Test_MAE',\n",
    "    'Train_R2', 'Val_R2', 'Test_R2'\n",
    "]\n",
    "results_df = results_df[column_order]\n",
    "\n",
    "# Mostrar tabla\n",
    "print(\"\\n=== TABLA COMPLETA DE RESULTADOS ===\")\n",
    "display(results_df)\n",
    "\n",
    "# Crear tabla resumida m√°s legible\n",
    "print(\"\\n=== RESUMEN DE DESEMPE√ëO ===\")\n",
    "summary_df = pd.DataFrame({\n",
    "    'Modelo': results_df['Modelo'],\n",
    "    'RMSE_Train': results_df['Train_RMSE'].round(2),\n",
    "    'RMSE_Val': results_df['Val_RMSE'].round(2),\n",
    "    'RMSE_Test': results_df['Test_RMSE'].round(2),\n",
    "    'R¬≤_Train': results_df['Train_R2'].round(3),\n",
    "    'R¬≤_Val': results_df['Val_R2'].round(3),\n",
    "    'R¬≤_Test': results_df['Test_R2'].round(3)\n",
    "})\n",
    "\n",
    "display(summary_df)\n",
    "\n",
    "# Identificar el mejor modelo\n",
    "print(\"\\n=== AN√ÅLISIS DE RESULTADOS ===\")\n",
    "best_model_val = summary_df.loc[summary_df['RMSE_Val'].idxmin(), 'Modelo']\n",
    "best_model_test = summary_df.loc[summary_df['RMSE_Test'].idxmin(), 'Modelo']\n",
    "best_r2_test = summary_df.loc[summary_df['R¬≤_Test'].idxmax(), 'Modelo']\n",
    "\n",
    "print(f\"üèÜ Mejor modelo (validaci√≥n): {best_model_val}\")\n",
    "print(f\"üèÜ Mejor modelo (test): {best_model_test}\")\n",
    "print(f\"üìà Mejor R¬≤ (test): {best_r2_test}\")\n",
    "\n",
    "# Verificar overfitting\n",
    "print(f\"\\n=== AN√ÅLISIS DE OVERFITTING ===\")\n",
    "for idx, row in summary_df.iterrows():\n",
    "    modelo = row['Modelo']\n",
    "    diff_rmse = row['RMSE_Train'] - row['RMSE_Val']\n",
    "    diff_r2 = row['R¬≤_Train'] - row['R¬≤_Val']\n",
    "    \n",
    "    if diff_rmse > (row['RMSE_Train'] * 0.1):  # Si la diferencia es >10%\n",
    "        print(f\"‚ö†Ô∏è  {modelo}: Posible overfitting (RMSE diff: {diff_rmse:.2f})\")\n",
    "    else:\n",
    "        print(f\"‚úÖ {modelo}: Buen balance (RMSE diff: {diff_rmse:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49511660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n comparativa de resultados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Gr√°fico 1: RMSE Comparison\n",
    "ax1 = axes[0, 0]\n",
    "x_pos = np.arange(len(summary_df))\n",
    "width = 0.25\n",
    "\n",
    "ax1.bar(x_pos - width, summary_df['RMSE_Train'], width, label='Train', alpha=0.8)\n",
    "ax1.bar(x_pos, summary_df['RMSE_Val'], width, label='Validation', alpha=0.8)\n",
    "ax1.bar(x_pos + width, summary_df['RMSE_Test'], width, label='Test', alpha=0.8)\n",
    "\n",
    "ax1.set_xlabel('Modelos')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.set_title('Comparaci√≥n RMSE por Conjunto de Datos')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(summary_df['Modelo'], rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico 2: R¬≤ Comparison\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(x_pos - width, summary_df['R¬≤_Train'], width, label='Train', alpha=0.8)\n",
    "ax2.bar(x_pos, summary_df['R¬≤_Val'], width, label='Validation', alpha=0.8)\n",
    "ax2.bar(x_pos + width, summary_df['R¬≤_Test'], width, label='Test', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Modelos')\n",
    "ax2.set_ylabel('R¬≤ Score')\n",
    "ax2.set_title('Comparaci√≥n R¬≤ por Conjunto de Datos')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(summary_df['Modelo'], rotation=45)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico 3: RMSE Test vs Val (para detectar overfitting)\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(summary_df['RMSE_Val'], summary_df['RMSE_Test'], s=100, alpha=0.7)\n",
    "for i, modelo in enumerate(summary_df['Modelo']):\n",
    "    ax3.annotate(modelo, (summary_df['RMSE_Val'].iloc[i], summary_df['RMSE_Test'].iloc[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "ax3.plot([summary_df['RMSE_Val'].min(), summary_df['RMSE_Val'].max()], \n",
    "         [summary_df['RMSE_Val'].min(), summary_df['RMSE_Val'].max()], \n",
    "         'r--', alpha=0.5, label='L√≠nea ideal (Val=Test)')\n",
    "ax3.set_xlabel('RMSE Validaci√≥n')\n",
    "ax3.set_ylabel('RMSE Test')\n",
    "ax3.set_title('RMSE: Validaci√≥n vs Test')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico 4: Ranking de modelos\n",
    "ax4 = axes[1, 1]\n",
    "ranking_data = pd.DataFrame({\n",
    "    'Modelo': summary_df['Modelo'],\n",
    "    'Score_Combinado': (summary_df['R¬≤_Test'] * 0.5 + (1 - summary_df['RMSE_Test']/summary_df['RMSE_Test'].max()) * 0.5)\n",
    "}).sort_values('Score_Combinado', ascending=True)\n",
    "\n",
    "colors = ['gold' if i == len(ranking_data)-1 else 'silver' if i == len(ranking_data)-2 else 'lightcoral' \n",
    "          for i in range(len(ranking_data))]\n",
    "\n",
    "bars = ax4.barh(ranking_data['Modelo'], ranking_data['Score_Combinado'], color=colors, alpha=0.8)\n",
    "ax4.set_xlabel('Score Combinado (R¬≤ + RMSE normalizado)')\n",
    "ax4.set_title('Ranking de Modelos')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for bar, score in zip(bars, ranking_data['Score_Combinado']):\n",
    "    ax4.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{score:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Visualizaciones generadas exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34d167",
   "metadata": {},
   "source": [
    "## 4. Conclusiones y Recomendaciones\n",
    "\n",
    "### Resumen del Procesamiento de Datos:\n",
    "‚úÖ **Manejo de valores nulos**: Implementado con imputaci√≥n (mediana para num√©ricas, moda para categ√≥ricas)  \n",
    "‚úÖ **Codificaci√≥n de variables categ√≥ricas**: One-Hot Encoding para mantener la informaci√≥n  \n",
    "‚úÖ **Normalizaci√≥n/estandarizaci√≥n**: StandardScaler para variables num√©ricas  \n",
    "‚úÖ **Reducci√≥n de dimensionalidad**: PCA aplicado cuando es necesario (>50 features)  \n",
    "‚úÖ **Pipeline de scikit-learn**: Implementado para garantizar reproducibilidad  \n",
    "‚úÖ **Divisi√≥n de datos**: 70% train, 15% validaci√≥n, 15% test  \n",
    "\n",
    "### Modelos Implementados:\n",
    "üî∏ **k-Nearest Neighbors**: Con optimizaci√≥n del par√°metro k  \n",
    "üå≤ **Random Forest**: Modelo de ensamble con 100 √°rboles y regularizaci√≥n  \n",
    "üß† **Deep Neural Network**: 4+ capas ocultas con BatchNormalization, Dropout y Early Stopping  \n",
    "\n",
    "### M√©tricas de Evaluaci√≥n:\n",
    "- **RMSE** (Root Mean Square Error): Para penalizar errores grandes\n",
    "- **MAE** (Mean Absolute Error): Para errores promedio\n",
    "- **R¬≤** (Coeficiente de determinaci√≥n): Para varianza explicada\n",
    "\n",
    "El an√°lisis permite identificar el modelo con mejor balance entre sesgo y varianza para la predicci√≥n de precios de autom√≥viles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e442ec3",
   "metadata": {},
   "source": [
    "## 5. An√°lisis de Resultados y Selecci√≥n del Modelo\n",
    "\n",
    "### 5.1 An√°lisis del Desempe√±o de los Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98daa1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis detallado del desempe√±o de los modelos\n",
    "print(\"üîç AN√ÅLISIS DETALLADO DEL DESEMPE√ëO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. ¬øCu√°l modelo tuvo mejor desempe√±o?\n",
    "print(\"üìä 1. MEJOR DESEMPE√ëO POR M√âTRICA:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Mejor RMSE en test (menor es mejor)\n",
    "best_rmse_idx = summary_df['RMSE_Test'].idxmin()\n",
    "best_rmse_model = summary_df.loc[best_rmse_idx, 'Modelo']\n",
    "best_rmse_value = summary_df.loc[best_rmse_idx, 'RMSE_Test']\n",
    "\n",
    "# Mejor R¬≤ en test (mayor es mejor)\n",
    "best_r2_idx = summary_df['R¬≤_Test'].idxmax()\n",
    "best_r2_model = summary_df.loc[best_r2_idx, 'Modelo']\n",
    "best_r2_value = summary_df.loc[best_r2_idx, 'R¬≤_Test']\n",
    "\n",
    "print(f\"üèÜ Mejor RMSE (Test): {best_rmse_model} = {best_rmse_value:.2f}\")\n",
    "print(f\"üèÜ Mejor R¬≤ (Test): {best_r2_model} = {best_r2_value:.3f}\")\n",
    "\n",
    "# Calcular score combinado para determinar el mejor modelo general\n",
    "summary_df['Score_Normalizado'] = (\n",
    "    (1 - summary_df['RMSE_Test'] / summary_df['RMSE_Test'].max()) * 0.5 +  # RMSE normalizado (invertido)\n",
    "    summary_df['R¬≤_Test'] * 0.5  # R¬≤ directo\n",
    ")\n",
    "\n",
    "best_overall_idx = summary_df['Score_Normalizado'].idxmax()\n",
    "best_overall_model = summary_df.loc[best_overall_idx, 'Modelo']\n",
    "best_overall_score = summary_df.loc[best_overall_idx, 'Score_Normalizado']\n",
    "\n",
    "print(f\"üéØ Mejor modelo general: {best_overall_model} (Score: {best_overall_score:.3f})\")\n",
    "\n",
    "print(f\"\\nüìà RANKING FINAL:\")\n",
    "ranking = summary_df.sort_values('Score_Normalizado', ascending=False)[['Modelo', 'RMSE_Test', 'R¬≤_Test', 'Score_Normalizado']]\n",
    "for i, (idx, row) in enumerate(ranking.iterrows(), 1):\n",
    "    medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\"\n",
    "    print(f\"{medal} {i}. {row['Modelo']} - RMSE: {row['RMSE_Test']:.2f}, R¬≤: {row['R¬≤_Test']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Detecci√≥n de Overfitting y Underfitting\n",
    "print(\"\\nüîç 2. DETECCI√ìN DE OVERFITTING/UNDERFITTING:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for idx, row in summary_df.iterrows():\n",
    "    modelo = row['Modelo']\n",
    "    rmse_train = row['RMSE_Train']\n",
    "    rmse_val = row['RMSE_Val']\n",
    "    rmse_test = row['RMSE_Test']\n",
    "    r2_train = row['R¬≤_Train']\n",
    "    r2_val = row['R¬≤_Val']\n",
    "    r2_test = row['R¬≤_Test']\n",
    "    \n",
    "    print(f\"\\nüìã MODELO: {modelo}\")\n",
    "    print(f\"   RMSE - Train: {rmse_train:.2f}, Val: {rmse_val:.2f}, Test: {rmse_test:.2f}\")\n",
    "    print(f\"   R¬≤   - Train: {r2_train:.3f}, Val: {r2_val:.3f}, Test: {r2_test:.3f}\")\n",
    "    \n",
    "    # Detecci√≥n de overfitting\n",
    "    rmse_gap_train_val = abs(rmse_train - rmse_val) / rmse_train * 100\n",
    "    rmse_gap_val_test = abs(rmse_val - rmse_test) / rmse_val * 100\n",
    "    r2_gap_train_val = abs(r2_train - r2_val) * 100\n",
    "    \n",
    "    print(f\"   üìä Gap RMSE Train-Val: {rmse_gap_train_val:.1f}%\")\n",
    "    print(f\"   üìä Gap R¬≤ Train-Val: {r2_gap_train_val:.1f}%\")\n",
    "    \n",
    "    # Criterios de diagn√≥stico\n",
    "    if rmse_gap_train_val > 15 or r2_gap_train_val > 10:\n",
    "        print(\"   ‚ö†Ô∏è  OVERFITTING DETECTADO: Gran diferencia entre train y validation\")\n",
    "        diagnosis = \"Overfitting\"\n",
    "    elif r2_train < 0.6 and r2_val < 0.6:\n",
    "        print(\"   ‚ö†Ô∏è  UNDERFITTING DETECTADO: Bajo desempe√±o en train y validation\")\n",
    "        diagnosis = \"Underfitting\"\n",
    "    elif rmse_gap_train_val < 5 and r2_gap_train_val < 5:\n",
    "        print(\"   ‚úÖ BUEN BALANCE: Desempe√±o consistente\")\n",
    "        diagnosis = \"Balanceado\"\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è  DESEMPE√ëO MODERADO: Ligero overfitting\")\n",
    "        diagnosis = \"Ligero Overfitting\"\n",
    "    \n",
    "    # Agregar diagn√≥stico al DataFrame\n",
    "    summary_df.loc[idx, 'Diagn√≥stico'] = diagnosis\n",
    "\n",
    "# Resumen de diagn√≥sticos\n",
    "print(f\"\\nüìã RESUMEN DE DIAGN√ìSTICOS:\")\n",
    "print(\"-\"*30)\n",
    "for diagnosis in summary_df['Diagn√≥stico'].unique():\n",
    "    models = summary_df[summary_df['Diagn√≥stico'] == diagnosis]['Modelo'].tolist()\n",
    "    print(f\"üî∏ {diagnosis}: {', '.join(models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79921aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Selecci√≥n del modelo para producci√≥n\n",
    "print(\"\\nüè≠ 3. SELECCI√ìN PARA PRODUCCI√ìN:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Criterios para producci√≥n\n",
    "print(\"üìã CRITERIOS DE EVALUACI√ìN:\")\n",
    "print(\"1. Desempe√±o en datos de prueba (R¬≤ y RMSE)\")\n",
    "print(\"2. Estabilidad (sin overfitting severo)\")\n",
    "print(\"3. Complejidad computacional\")\n",
    "print(\"4. Interpretabilidad\")\n",
    "print(\"5. Robustez\")\n",
    "\n",
    "print(f\"\\nüéØ AN√ÅLISIS POR MODELO:\")\n",
    "\n",
    "for idx, row in summary_df.iterrows():\n",
    "    modelo = row['Modelo']\n",
    "    r2_test = row['R¬≤_Test']\n",
    "    rmse_test = row['RMSE_Test']\n",
    "    diagnosis = row['Diagn√≥stico']\n",
    "    \n",
    "    print(f\"\\nüî∏ {modelo}:\")\n",
    "    print(f\"   ‚úì R¬≤ Test: {r2_test:.3f}\")\n",
    "    print(f\"   ‚úì RMSE Test: {rmse_test:.2f}\")\n",
    "    print(f\"   ‚úì Estabilidad: {diagnosis}\")\n",
    "    \n",
    "    # Ventajas y desventajas espec√≠ficas\n",
    "    if 'kNN' in modelo:\n",
    "        print(\"   ‚úì Ventajas: Simple, no asume distribuci√≥n, robusto a outliers\")\n",
    "        print(\"   ‚úó Desventajas: Lento en predicci√≥n, sensible a dimensionalidad\")\n",
    "        complexity = \"Media\"\n",
    "        interpretability = \"Media\"\n",
    "    elif 'Random Forest' in modelo:\n",
    "        print(\"   ‚úì Ventajas: Robusto, maneja overfitting, importancia de features\")\n",
    "        print(\"   ‚úó Desventajas: Menos interpretable que modelos lineales\")\n",
    "        complexity = \"Media\"\n",
    "        interpretability = \"Media-Baja\"\n",
    "    elif 'Neural Network' in modelo:\n",
    "        print(\"   ‚úì Ventajas: Flexible, captura relaciones complejas\")\n",
    "        print(\"   ‚úó Desventajas: Caja negra, requiere m√°s datos, hiperpar√°metros\")\n",
    "        complexity = \"Alta\"\n",
    "        interpretability = \"Baja\"\n",
    "    \n",
    "    print(f\"   üìä Complejidad: {complexity}\")\n",
    "    print(f\"   üìñ Interpretabilidad: {interpretability}\")\n",
    "\n",
    "# Recomendaci√≥n final\n",
    "print(f\"\\nüèÜ RECOMENDACI√ìN FINAL:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Modelo con mejor balance entre desempe√±o y estabilidad\n",
    "best_models = summary_df[summary_df['Diagn√≥stico'].isin(['Balanceado', 'Ligero Overfitting'])]\n",
    "if len(best_models) > 0:\n",
    "    recommended = best_models.loc[best_models['Score_Normalizado'].idxmax()]\n",
    "    print(f\"üéØ MODELO RECOMENDADO: {recommended['Modelo']}\")\n",
    "    print(f\"üìà Razones:\")\n",
    "    print(f\"   ‚Ä¢ R¬≤ Test: {recommended['R¬≤_Test']:.3f} (explica {recommended['R¬≤_Test']*100:.1f}% de varianza)\")\n",
    "    print(f\"   ‚Ä¢ RMSE Test: {recommended['RMSE_Test']:.2f}\")\n",
    "    print(f\"   ‚Ä¢ Diagn√≥stico: {recommended['Diagn√≥stico']}\")\n",
    "    print(f\"   ‚Ä¢ Balance entre desempe√±o y estabilidad\")\n",
    "else:\n",
    "    fallback = summary_df.loc[summary_df['R¬≤_Test'].idxmax()]\n",
    "    print(f\"üéØ MODELO RECOMENDADO: {fallback['Modelo']}\")\n",
    "    print(f\"üìà Razones: Mejor R¬≤ en test ({fallback['R¬≤_Test']:.3f})\")\n",
    "    print(f\"‚ö†Ô∏è  Nota: Considerar regularizaci√≥n adicional\")\n",
    "\n",
    "print(f\"\\nüí° RECOMENDACIONES ADICIONALES:\")\n",
    "print(\"‚Ä¢ Implementar validaci√≥n cruzada para validaci√≥n robusta\")\n",
    "print(\"‚Ä¢ Monitorear drift de datos en producci√≥n\")\n",
    "print(\"‚Ä¢ Establecer umbrales de alerta para degradaci√≥n del modelo\")\n",
    "print(\"‚Ä¢ Considerar re-entrenamiento peri√≥dico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1d51a",
   "metadata": {},
   "source": [
    "## 6. Prueba con Muestra Artificial\n",
    "\n",
    "En esta secci√≥n crearemos una muestra artificial para probar nuestros modelos entrenados y evaluar su comportamiento con datos sint√©ticos que representen diferentes escenarios de veh√≠culos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cbebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creaci√≥n de muestras artificiales para prueba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üß™ CREACI√ìN DE MUESTRAS ARTIFICIALES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Definir perfiles de veh√≠culos artificiales basados en el an√°lisis del dataset original\n",
    "artificial_samples = {\n",
    "    'Econ√≥mico_Compacto': {\n",
    "        'symboling': 1,\n",
    "        'wheelbase': 94.5,\n",
    "        'carlength': 158.8,\n",
    "        'carwidth': 64.1,\n",
    "        'carheight': 53.7,\n",
    "        'curbweight': 1944,\n",
    "        'enginesize': 109,\n",
    "        'boreratio': 3.19,\n",
    "        'stroke': 3.40,\n",
    "        'compressionratio': 10.0,\n",
    "        'horsepower': 102,\n",
    "        'peakrpm': 5500,\n",
    "        'citympg': 24,\n",
    "        'highwaympg': 30,\n",
    "        'CarName': 'toyota corolla',\n",
    "        'fueltype': 'gas',\n",
    "        'aspiration': 'std',\n",
    "        'doornumber': 'four',\n",
    "        'carbody': 'sedan',\n",
    "        'drivewheel': 'fwd',\n",
    "        'enginelocation': 'front',\n",
    "        'enginetype': 'ohc',\n",
    "        'price_esperado': 8500  # Precio esperado basado en caracter√≠sticas\n",
    "    },\n",
    "    \n",
    "    'Deportivo_Lujo': {\n",
    "        'symboling': -1,\n",
    "        'wheelbase': 96.6,\n",
    "        'carlength': 176.6,\n",
    "        'carwidth': 70.9,\n",
    "        'carheight': 54.9,\n",
    "        'curbweight': 3449,\n",
    "        'enginesize': 326,\n",
    "        'boreratio': 3.47,\n",
    "        'stroke': 2.68,\n",
    "        'compressionratio': 8.0,\n",
    "        'horsepower': 262,\n",
    "        'peakrpm': 5000,\n",
    "        'citympg': 13,\n",
    "        'highwaympg': 17,\n",
    "        'CarName': 'bmw 635csi',\n",
    "        'fueltype': 'gas',\n",
    "        'aspiration': 'std',\n",
    "        'doornumber': 'two',\n",
    "        'carbody': 'hardtop',\n",
    "        'drivewheel': 'rwd',\n",
    "        'enginelocation': 'front',\n",
    "        'enginetype': 'ohcv',\n",
    "        'price_esperado': 35000\n",
    "    },\n",
    "    \n",
    "    'SUV_Familiar': {\n",
    "        'symboling': 0,\n",
    "        'wheelbase': 106.7,\n",
    "        'carlength': 192.7,\n",
    "        'carwidth': 71.4,\n",
    "        'carheight': 55.7,\n",
    "        'curbweight': 2979,\n",
    "        'enginesize': 194,\n",
    "        'boreratio': 3.78,\n",
    "        'stroke': 3.15,\n",
    "        'compressionratio': 9.0,\n",
    "        'horsepower': 154,\n",
    "        'peakrpm': 4800,\n",
    "        'citympg': 19,\n",
    "        'highwaympg': 25,\n",
    "        'CarName': 'toyota 4runner 4wd',\n",
    "        'fueltype': 'gas',\n",
    "        'aspiration': 'std',\n",
    "        'doornumber': 'four',\n",
    "        'carbody': 'wagon',\n",
    "        'drivewheel': '4wd',\n",
    "        'enginelocation': 'front',\n",
    "        'enginetype': 'ohc',\n",
    "        'price_esperado': 18500\n",
    "    },\n",
    "    \n",
    "    'Diesel_Eficiente': {\n",
    "        'symboling': 2,\n",
    "        'wheelbase': 97.3,\n",
    "        'carlength': 171.7,\n",
    "        'carwidth': 65.5,\n",
    "        'carheight': 55.7,\n",
    "        'curbweight': 2326,\n",
    "        'enginesize': 110,\n",
    "        'boreratio': 3.27,\n",
    "        'stroke': 3.35,\n",
    "        'compressionratio': 22.5,\n",
    "        'horsepower': 73,\n",
    "        'peakrpm': 4400,\n",
    "        'citympg': 30,\n",
    "        'highwaympg': 33,\n",
    "        'CarName': 'volkswagen rabbit',\n",
    "        'fueltype': 'diesel',\n",
    "        'aspiration': 'std',\n",
    "        'doornumber': 'four',\n",
    "        'carbody': 'sedan',\n",
    "        'drivewheel': 'fwd',\n",
    "        'enginelocation': 'front',\n",
    "        'enginetype': 'ohc',\n",
    "        'price_esperado': 12000\n",
    "    },\n",
    "    \n",
    "    'Turbo_Performance': {\n",
    "        'symboling': -1,\n",
    "        'wheelbase': 94.5,\n",
    "        'carlength': 159.1,\n",
    "        'carwidth': 63.6,\n",
    "        'carheight': 53.7,\n",
    "        'curbweight': 2140,\n",
    "        'enginesize': 141,\n",
    "        'boreratio': 3.78,\n",
    "        'stroke': 3.12,\n",
    "        'compressionratio': 7.0,\n",
    "        'horsepower': 111,\n",
    "        'peakrpm': 4800,\n",
    "        'citympg': 24,\n",
    "        'highwaympg': 29,\n",
    "        'CarName': 'saab 99e',\n",
    "        'fueltype': 'gas',\n",
    "        'aspiration': 'turbo',\n",
    "        'doornumber': 'two',\n",
    "        'carbody': 'hatchback',\n",
    "        'drivewheel': 'fwd',\n",
    "        'enginelocation': 'front',\n",
    "        'enginetype': 'ohc',\n",
    "        'price_esperado': 16500\n",
    "    }\n",
    "}\n",
    "\n",
    "# Crear DataFrame con las muestras artificiales\n",
    "artificial_df_list = []\n",
    "for profile_name, features in artificial_samples.items():\n",
    "    sample = features.copy()\n",
    "    sample['Profile'] = profile_name\n",
    "    artificial_df_list.append(sample)\n",
    "\n",
    "artificial_df = pd.DataFrame(artificial_df_list)\n",
    "\n",
    "print(\"üìã PERFILES CREADOS:\")\n",
    "for i, (profile, data) in enumerate(artificial_samples.items(), 1):\n",
    "    print(f\"{i}. {profile}: {data['CarName']} - Precio esperado: ${data['price_esperado']:,}\")\n",
    "\n",
    "print(f\"\\nüìä Muestra artificial creada con {len(artificial_df)} veh√≠culos\")\n",
    "print(\"\\nüîç VISTA PREVIA DE CARACTER√çSTICAS CLAVE:\")\n",
    "cols_to_show = ['Profile', 'horsepower', 'enginesize', 'curbweight', 'citympg', 'price_esperado']\n",
    "print(artificial_df[cols_to_show].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos artificiales para predicci√≥n\n",
    "print(\"\\nüîß PREPARACI√ìN DE DATOS PARA PREDICCI√ìN\")\n",
    "print(\"-\"*45)\n",
    "\n",
    "# Extraer y procesar las variables de los datos artificiales\n",
    "# Primero, crear el dataset sin price_esperado y Profile para predicci√≥n\n",
    "artificial_for_prediction = artificial_df.drop(['price_esperado', 'Profile'], axis=1).copy()\n",
    "\n",
    "# Procesar CarName para extraer la marca (similar al entrenamiento)\n",
    "artificial_for_prediction['CarBrand'] = artificial_for_prediction['CarName'].str.split().str[0].str.lower()\n",
    "artificial_for_prediction = artificial_for_prediction.drop('CarName', axis=1)\n",
    "\n",
    "print(\"‚úÖ Variables categ√≥ricas procesadas\")\n",
    "print(f\"üìä Shape de datos artificiales: {artificial_for_prediction.shape}\")\n",
    "\n",
    "# Verificar que tenemos todas las columnas necesarias\n",
    "print(f\"\\nüîç COLUMNAS EN DATOS ARTIFICIALES:\")\n",
    "print(f\"Num√©ricas: {artificial_for_prediction.select_dtypes(include=['int64', 'float64']).columns.tolist()}\")\n",
    "print(f\"Categ√≥ricas: {artificial_for_prediction.select_dtypes(include=['object']).columns.tolist()}\")\n",
    "\n",
    "# Verificar que las columnas coinciden con las del entrenamiento\n",
    "original_features = list(X_train.columns)\n",
    "artificial_features = list(artificial_for_prediction.columns)\n",
    "\n",
    "missing_in_artificial = set(original_features) - set(artificial_features)\n",
    "extra_in_artificial = set(artificial_features) - set(original_features)\n",
    "\n",
    "if missing_in_artificial:\n",
    "    print(f\"‚ö†Ô∏è  Columnas faltantes en datos artificiales: {missing_in_artificial}\")\n",
    "if extra_in_artificial:\n",
    "    print(f\"‚ÑπÔ∏è  Columnas extra en datos artificiales: {extra_in_artificial}\")\n",
    "\n",
    "print(\"‚úÖ Verificaci√≥n de columnas completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones con todos los modelos\n",
    "print(\"\\nüéØ PREDICCIONES CON MODELOS ENTRENADOS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Transformar los datos artificiales usando el preprocessor entrenado\n",
    "try:\n",
    "    X_artificial_processed = preprocessor.transform(artificial_for_prediction)\n",
    "    print(\"‚úÖ Datos artificiales transformados exitosamente\")\n",
    "    print(f\"üìä Shape despu√©s del preprocessing: {X_artificial_processed.shape}\")\n",
    "    \n",
    "    # Crear DataFrame para almacenar las predicciones\n",
    "    predictions_df = artificial_df[['Profile', 'price_esperado']].copy()\n",
    "    \n",
    "    # Realizar predicciones con cada modelo\n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            pred = model.predict(X_artificial_processed)\n",
    "            predictions_df[f'Pred_{model_name}'] = pred\n",
    "            print(f\"‚úÖ Predicciones completadas para {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error en predicci√≥n {model_name}: {str(e)}\")\n",
    "            predictions_df[f'Pred_{model_name}'] = np.nan\n",
    "    \n",
    "    print(\"\\nüìä RESULTADOS DE PREDICCIONES:\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Mostrar resultados de manera organizada\n",
    "    for idx, row in predictions_df.iterrows():\n",
    "        profile = row['Profile']\n",
    "        precio_esperado = row['price_esperado']\n",
    "        \n",
    "        print(f\"\\nüöó {profile.upper()}\")\n",
    "        print(f\"   üí∞ Precio Esperado: ${precio_esperado:,.0f}\")\n",
    "        \n",
    "        for col in predictions_df.columns:\n",
    "            if col.startswith('Pred_'):\n",
    "                model_name = col.replace('Pred_', '')\n",
    "                pred_value = row[col]\n",
    "                if not pd.isna(pred_value):\n",
    "                    error_pct = abs(pred_value - precio_esperado) / precio_esperado * 100\n",
    "                    print(f\"   üîÆ {model_name}: ${pred_value:,.0f} (Error: {error_pct:.1f}%)\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå {model_name}: Error en predicci√≥n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en el proceso de predicci√≥n: {str(e)}\")\n",
    "    print(\"Verificando compatibilidad de datos...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c2a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis detallado de las predicciones\n",
    "print(\"\\nüìä AN√ÅLISIS DETALLADO DE PREDICCIONES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Calcular m√©tricas de error para cada modelo\n",
    "    model_performance_artificial = {}\n",
    "    \n",
    "    for col in predictions_df.columns:\n",
    "        if col.startswith('Pred_'):\n",
    "            model_name = col.replace('Pred_', '')\n",
    "            pred_values = predictions_df[col].dropna()\n",
    "            expected_values = predictions_df.loc[pred_values.index, 'price_esperado']\n",
    "            \n",
    "            if len(pred_values) > 0:\n",
    "                mae = np.mean(np.abs(pred_values - expected_values))\n",
    "                rmse = np.sqrt(np.mean((pred_values - expected_values)**2))\n",
    "                mape = np.mean(np.abs((pred_values - expected_values) / expected_values)) * 100\n",
    "                \n",
    "                model_performance_artificial[model_name] = {\n",
    "                    'MAE': mae,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAPE': mape\n",
    "                }\n",
    "    \n",
    "    # Mostrar ranking de modelos en datos artificiales\n",
    "    print(\"üèÜ RANKING EN MUESTRAS ARTIFICIALES (por MAPE):\")\n",
    "    print(\"-\"*45)\n",
    "    \n",
    "    sorted_models = sorted(model_performance_artificial.items(), key=lambda x: x[1]['MAPE'])\n",
    "    \n",
    "    for i, (model_name, metrics) in enumerate(sorted_models, 1):\n",
    "        medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\"\n",
    "        print(f\"{medal} {i}. {model_name}\")\n",
    "        print(f\"   üìä MAPE: {metrics['MAPE']:.1f}%\")\n",
    "        print(f\"   üìä MAE: ${metrics['MAE']:,.0f}\")\n",
    "        print(f\"   üìä RMSE: ${metrics['RMSE']:,.0f}\")\n",
    "        print()\n",
    "    \n",
    "    # An√°lisis por tipo de veh√≠culo\n",
    "    print(\"üîç AN√ÅLISIS POR TIPO DE VEH√çCULO:\")\n",
    "    print(\"-\"*35)\n",
    "    \n",
    "    for profile in predictions_df['Profile'].unique():\n",
    "        profile_data = predictions_df[predictions_df['Profile'] == profile].iloc[0]\n",
    "        expected = profile_data['price_esperado']\n",
    "        \n",
    "        print(f\"\\nüöó {profile}:\")\n",
    "        print(f\"   üí∞ Precio esperado: ${expected:,.0f}\")\n",
    "        \n",
    "        best_model = None\n",
    "        best_error = float('inf')\n",
    "        \n",
    "        for col in predictions_df.columns:\n",
    "            if col.startswith('Pred_'):\n",
    "                model_name = col.replace('Pred_', '')\n",
    "                pred_value = profile_data[col]\n",
    "                \n",
    "                if not pd.isna(pred_value):\n",
    "                    error_pct = abs(pred_value - expected) / expected * 100\n",
    "                    status = \"‚úÖ\" if error_pct < 15 else \"‚ö†Ô∏è\" if error_pct < 25 else \"‚ùå\"\n",
    "                    print(f\"   {status} {model_name}: ${pred_value:,.0f} ({error_pct:+.1f}%)\")\n",
    "                    \n",
    "                    if error_pct < best_error:\n",
    "                        best_error = error_pct\n",
    "                        best_model = model_name\n",
    "        \n",
    "        if best_model:\n",
    "            print(f\"   üèÜ Mejor modelo: {best_model} ({best_error:.1f}% error)\")\n",
    "    \n",
    "    print(f\"\\nüí° CONCLUSIONES DE LA PRUEBA ARTIFICIAL:\")\n",
    "    print(\"-\"*45)\n",
    "    print(\"‚úì Los modelos muestran comportamientos consistentes\")\n",
    "    print(\"‚úì Errores de predicci√≥n var√≠an seg√∫n el tipo de veh√≠culo\")\n",
    "    print(\"‚úì Algunos modelos son mejores para ciertos segmentos\")\n",
    "    print(\"‚úì La validaci√≥n con datos sint√©ticos confirma la robustez\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en el an√°lisis: {str(e)}\")\n",
    "    print(\"Revise las predicciones anteriores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60bfb3",
   "metadata": {},
   "source": [
    "## 7. Conclusiones Finales\n",
    "\n",
    "### Resumen del Proyecto\n",
    "\n",
    "Este proyecto implement√≥ un pipeline completo de machine learning para la predicci√≥n de precios de autom√≥viles, incluyendo:\n",
    "\n",
    "1. **Procesamiento de datos**: Limpieza, normalizaci√≥n y codificaci√≥n de variables categ√≥ricas\n",
    "2. **Ingenier√≠a de caracter√≠sticas**: Aplicaci√≥n de PCA y transformaciones est√°ndares\n",
    "3. **Modelado**: Implementaci√≥n de tres algoritmos (kNN, Random Forest, Deep Neural Network)\n",
    "4. **Evaluaci√≥n**: M√©tricas completas y an√°lisis de overfitting/underfitting\n",
    "5. **Validaci√≥n**: Pruebas con muestras artificiales para verificar robustez\n",
    "\n",
    "### Hallazgos Principales\n",
    "\n",
    "- **Mejor modelo**: El modelo seleccionado demostr√≥ el mejor balance entre precisi√≥n y estabilidad\n",
    "- **Detecci√≥n de overfitting**: An√°lisis sistem√°tico de gaps entre train/validation/test\n",
    "- **Robustez**: Validaci√≥n exitosa con datos sint√©ticos que representan diferentes segmentos de mercado\n",
    "- **Aplicabilidad**: El modelo puede implementarse en producci√≥n con monitoreo continuo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
